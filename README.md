# Common-Misconceptions

The objective is to compare different static analysis tools in their ability to detect symptoms of misconception in students' code. For each misconception, we make a table comparing the effectiveness of each tools, using the following criterias :

- **Accuracy** (in terms of false positives/negatives)
- **Performance** : How long does it take to analyze `x` snippets of code ?
- **Expressiveness (Readability)** : How complicated is it to understand what the implementation does ?
- **Expressiveness (Writability)** : How complicated is it to implement the misconception using the tool ?

|| Flake8 | Regex | CodeQL |
|---|---|---|---|
Accuracy | | | |
Performance | | | |
Readability | | | |
Writability | | | |

Finally, we'll compare each tools in a more "general" approach by condensing the observations from all the tables into a final one. We'll be adding a few criterias that cannot really be explored for each misconception :

- **Learning curve**
- **Output quality**
- **Setup complexity**
- **Usability** : Are there many symptoms that cannot be detected by the tool ?

|| Flake8 | Regex | CodeQL |
|---|---|---|---|
Accuracy | | | |
Performance | | | |
Readability | | | |
Writability | | | |
Learning curve | | | |
Output quality | | | |
Usability | | | |
